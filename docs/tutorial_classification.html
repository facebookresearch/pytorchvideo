<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Training a PyTorchVideo classification model · PyTorchVideo</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="# Introduction"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Training a PyTorchVideo classification model · PyTorchVideo"/><meta property="og:type" content="website"/><meta property="og:url" content="https://kalyanvasudev.github.io/ptv_tempindex"/><meta property="og:description" content="# Introduction"/><meta property="og:image" content="https://kalyanvasudev.github.io/ptv_temp/img/logo.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://kalyanvasudev.github.io/ptv_temp/img/logo.svg"/><link rel="shortcut icon" href="/ptv_tempimg/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/ptv_tempjs/scrollSpy.js"></script><link rel="stylesheet" href="/ptv_tempcss/main.css"/><script src="/ptv_tempjs/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/ptv_temp"><img class="logo" src="/ptv_tempimg/logo.svg" alt="PyTorchVideo"/><h2 class="headerTitleWithLogo">PyTorchVideo</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/ptv_tempdocs/tutorial_overview" target="_self">Tutorials</a></li><li class=""><a href="https://ptv-temp.readthedocs.io/en/latest/index.html" target="_self">Docs</a></li><li class=""><a href="https://github.com/facebookresearch/pytorchvideo/" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Classification</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Tutorials</h3><ul class=""><li class="navListItem"><a class="navItem" href="/ptv_tempdocs/tutorial_overview">Overview</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Classification</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/ptv_tempdocs/tutorial_classification">Training a PyTorchVideo classification model</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Accelerator</h3><ul class=""><li class="navListItem"><a class="navItem" href="/ptv_tempdocs/doc_accelerator_overview">Overview</a></li><li class="navListItem"><a class="navItem" href="/ptv_tempdocs/tutorial_accelerator_build_your_model">Build your efficient model with PytorchVideo/Accelerator</a></li><li class="navListItem"><a class="navItem" href="/ptv_tempdocs/tutorial_accelerator_use_accelerator_model_zoo">Use PytorchVideo/Accelerator Model Zoo</a></li><li class="navListItem"><a class="navItem" href="/ptv_tempdocs/tutorial_accelerator_use_model_transmuter">Accelerate your model with model transmuter in PytorchVideo/Accelerator</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Training a PyTorchVideo classification model</h1></header><article><div><span><h1><a class="anchor" aria-hidden="true" id="introduction"></a><a href="#introduction" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h1>
<p>In this tutorial we will show how to build a simple video classification training pipeline using PyTorchVideo models, datasets and transforms. We'll be using a 3D ResNet [1] for the model, Kinetics [2] for the dataset and a standard video transform augmentation recipe. As PyTorchVideo doesn't contain training code, we'll use <a href="https://github.com/PyTorchLightning/pytorch-lightning">PyTorch Lightning</a> - a lightweight PyTorch training framework - to help out. Don't worry if you don't have Lightning experience, we'll explain what's needed as we go along.</p>
<p>[1] He, Kaiming, et al. Deep Residual Learning for Image Recognition. ArXiv:1512.03385, 2015.</p>
<p>[2] W. Kay, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.</p>
<h1><a class="anchor" aria-hidden="true" id="dataset"></a><a href="#dataset" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dataset</h1>
<p>To start off with, let's setup the PyTorchVideo Kinetics data loader using a <a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule">pytorch_lightning_LightningDataModule</a> . A LightningDataModule is a wrapper that defines the train, val and test data partitions, we'll use it to wrap the PyTorchVideo Kinetics dataset below.</p>
<p>The PyTorchVideo Kinetics dataset is just an alias for the general <a href="http://pytorchvideo.org/api/data/encoded_video.html#pytorchvideo.data.encoded_video_dataset.EncodedVideoDataset">pytorchvideo.data.EncodedVideoDataset</a> class. If you look at its constructor, you'll notice that most args are what you'd expect (e.g. path to data). However, there are a few args that are more specific to PyTorchVideo datasets:</p>
<ul>
<li>video_sampler - defining the order to sample a video at each iteration. The default is a &quot;random&quot;.</li>
<li>clip_sampler - defining how to sample a clip from the chosen video at each iteration. For a train partition it is typical to use a &quot;random&quot; clip sampler (i.e. take a random clip of the specified duration from the video). For testing, typically you'll use &quot;uniform&quot; (i.e. uniformly sample all clips of the specified duration from the video) to ensure the entire video is sampled in each epoch.</li>
<li>transform - this provides a way to apply user defined data preprocessing or augmentation before batch collating by the PyTorch data loader. We'll show an example using this later.</li>
</ul>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pytorch_lightning
<span class="hljs-keyword">import</span> pytorchvideo.data
<span class="hljs-keyword">import</span> torch.utils.data

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">KineticsDataModule</span><span class="hljs-params">(pytorch_lightning.LightningDataModule)</span>:</span>

  <span class="hljs-comment"># Dataset configuration</span>
  _DATA_PATH = &lt;path_to_kinetics_data_dir&gt;
  _CLIP_DURATION = <span class="hljs-number">2</span>  <span class="hljs-comment"># Duration of sampled clip for each video</span>
  _BATCH_SIZE = <span class="hljs-number">8</span>
  _NUM_WORKERS = <span class="hljs-number">8</span>  <span class="hljs-comment"># Number of parallel processes fetching data</span>

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_dataloader</span><span class="hljs-params">(self)</span>:</span>
    <span class="hljs-string">"""
    Create the Kinetics train partition from the list of video labels
    in {self._DATA_PATH}/train.csv
    """</span>
    train_dataset = pytorchvideo.data.Kinetics(
        data_path=os.path.join(self._DATA_PATH, <span class="hljs-string">"train.csv"</span>),
        clip_sampler=pytorchvideo.data.make_clip_sampler(<span class="hljs-string">"random"</span>, self._CLIP_DURATION),
    )
    <span class="hljs-keyword">return</span> torch.utils.data.DataLoader(
        train_dataset,
        batch_size=self._BATCH_SIZE,
        num_workers=self._NUM_WORKERS,
    )

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">val_dataloader</span><span class="hljs-params">(self)</span>:</span>
    <span class="hljs-string">"""
    Create the Kinetics validation partition from the list of video labels
    in {self._DATA_PATH}/train.csv
    """</span>
    val_dataset = pytorchvideo.data.Kinetics(
        data_path=os.path.join(self._DATA_PATH, <span class="hljs-string">"val.csv"</span>),
        clip_sampler=pytorchvideo.data.make_clip_sampler(<span class="hljs-string">"uniform"</span>, self._CLIP_DURATION),
    )
    <span class="hljs-keyword">return</span> torch.utils.data.DataLoader(
        val_dataset,
        batch_size=self._BATCH_SIZE,
        num_workers=self._NUM_WORKERS,
    )
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="transforms"></a><a href="#transforms" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transforms</h1>
<p>As mentioned above, PyTorchVideo datasets take a &quot;transform&quot; callable arg that defines custom processing (e.g. augmentations, normalization) that's applied to each clip. The callable arg takes a clip dictionary defining the different modalities and metadata. pytorchvideo.data.Kinetics clips have the following dictionary format:</p>
<pre><code class="hljs css language-python">  {
     <span class="hljs-string">'video'</span>: &lt;video_tensor&gt;,     <span class="hljs-comment"># Shape: (C, T, H, W)</span>
     <span class="hljs-string">'audio'</span>: &lt;audio_tensor&gt;,     <span class="hljs-comment"># Shape: (S)</span>
     <span class="hljs-string">'label'</span>: &lt;action_label&gt;,     <span class="hljs-comment"># Integer defining class annotation</span>
     <span class="hljs-string">'video_name'</span>: &lt;video_path&gt;,  <span class="hljs-comment"># Video file path stem</span>
     <span class="hljs-string">'video_index'</span>: &lt;video_id&gt;,   <span class="hljs-comment"># index of video used by sampler</span>
     <span class="hljs-string">'clip_index'</span>: &lt;clip_id&gt;      <span class="hljs-comment"># index of clip sampled within video</span>
  }
</code></pre>
<p>PyTorchVideo provides several transforms which you can see in the <a href="http://pytorchvideo.org/api/transforms/transforms.html">docs</a> Notably, PyTorchVideo provides dictionary transforms that can be used to easily interoperate with other domain specifc libraries. For example, <a href="http://pytorchvideo.org/api/transforms/transforms.html#pytorchvideo.transforms.transforms.ApplyTransformToKey">pytorchvideo.transforms.ApplyTransformToKey(key, transform)</a>, can be used to apply domain specific transforms to a specific dictionary key. For video tensors we use the same tensor shape as TorchVision and for audio we use TorchAudio tensor shapes, making it east to apply their transforms alongside PyTorchVideo ones.</p>
<p>Below we revise the LightningDataModule from the last section to include transforms coming from both TorchVision and PyTorchVideo. For brevity we'll just show the KineticsDataModule.train_dataloader method. The validation dataset transforms would be the same just without the augmentations (RandomShortSideScale, RandomCropVideo, RandomHorizontalFlipVideo).</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">from</span> pytorchvideo.transforms <span class="hljs-keyword">import</span> (
    ApplyTransformToKey,
    RandomShortSideScale,
    RemoveKey,
    ShortSideScale,
    UniformTemporalSubsample
)

<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> (
    Compose,
    Normalize,
    RandomCrop,
    RandomHorizontalFlip
)

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">KineticsDataModule</span><span class="hljs-params">(pytorch_lightning.LightningDataModule)</span>:</span>

<span class="hljs-comment"># ...</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_dataloader</span><span class="hljs-params">(self)</span>:</span>
      <span class="hljs-string">"""
        Create the Kinetics train partition from the list of video labels
        in {self._DATA_PATH}/train.csv. Add transform that subsamples and
        normalizes the video before applying the scale, crop and flip augmentations.
        """</span>
        train_transform = Compose(
            [
            ApplyTransformToKey(
              key=<span class="hljs-string">"video"</span>,
              transform=Compose(
                  [
                    UniformTemporalSubsample(<span class="hljs-number">8</span>),
                    Normalize((<span class="hljs-number">0.45</span>, <span class="hljs-number">0.45</span>, <span class="hljs-number">0.45</span>), (<span class="hljs-number">0.225</span>, <span class="hljs-number">0.225</span>, <span class="hljs-number">0.225</span>)),
                    RandomShortSideScale(min_size=<span class="hljs-number">256</span>, max_size=<span class="hljs-number">320</span>),
                    RandomCrop(<span class="hljs-number">244</span>),
                    RandomHorizontalFlip(p=<span class="hljs-number">0.5</span>),
                  ]
                ),
              ),
            ]
        )
        train_dataset = pytorchvideo.data.Kinetics(
            data_path=os.path.join(self._DATA_PATH, <span class="hljs-string">"train.csv"</span>),
            clip_sampler=pytorchvideo.data.make_clip_sampler(<span class="hljs-string">"random"</span>, self._CLIP_DURATION),
            transform=train_transform
        )
        <span class="hljs-keyword">return</span> torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self._BATCH_SIZE,
            num_workers=self._NUM_WORKERS,
        )

<span class="hljs-comment"># ...</span>

</code></pre>
<h1><a class="anchor" aria-hidden="true" id="model"></a><a href="#model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model</h1>
<p>All PyTorchVideo models and layers can be built with simple, reproducible factory functions. We call this the &quot;flat&quot; model interface because the args don't require hierachies of configs to be used. An example building a default ResNet can be found below. See the <a href="http://pytorchvideo.org/api/models/resnet.html#pytorchvideo.models.resnet.create_bottleneck_block">docs</a> for more configuration options.</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> pytorchvideo.models.resnet

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_kinetics_resnet</span><span class="hljs-params">()</span>:</span>
  <span class="hljs-keyword">return</span> pytorchvideo.models.resnet.create_resnet(
      input_channel=<span class="hljs-number">3</span>, <span class="hljs-comment"># RGB input from Kinetics</span>
      model_depth=<span class="hljs-number">50</span>, <span class="hljs-comment"># For the tutorial let's just use a 50 layer network</span>
      model_num_class=<span class="hljs-number">400</span>, <span class="hljs-comment"># Kinetics has 400 classes so we need out final head to align</span>
      norm=nn.BatchNorm3d,
      activation=nn.ReLU,
  )
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="putting-it-all-together"></a><a href="#putting-it-all-together" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Putting it all together</h1>
<p>To put everything together, let's create a <a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html">pytorch_lightning.LightningModule</a>. This defines the train and validation step code (i.e. the code inside the training and evaluation loops), and the optimizer.</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VideoClassificationLightningModule</span><span class="hljs-params">(pytorch_lightning.LightningModule)</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
      super().__init__()
      self.model = make_kinetics_resnet()

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
      <span class="hljs-keyword">return</span> self.model(x)

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">training_step</span><span class="hljs-params">(self, batch, batch_idx)</span>:</span>
      <span class="hljs-comment"># The model expects a video tensor of shape (B, C, T, H, W), which is the </span>
      <span class="hljs-comment"># format provided by the dataset</span>
      y_hat = self.model(batch[<span class="hljs-string">"video"</span>])

      <span class="hljs-comment"># Compute cross entropy loss, loss.backwards will be called behind the scenes</span>
      <span class="hljs-comment"># by PyTorchLightning after being returned from this method.</span>
      loss = F.cross_entropy(y_hat, batch[<span class="hljs-string">"label"</span>])

      <span class="hljs-comment"># Log the train loss to Tensorboard</span>
      self.log(<span class="hljs-string">"train_loss"</span>, loss.item())

      <span class="hljs-keyword">return</span> loss

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validation_step</span><span class="hljs-params">(self, batch, batch_idx)</span>:</span>
      y_hat = self.model(batch[<span class="hljs-string">"video"</span>])
      loss = F.cross_entropy(y_hat, batch[<span class="hljs-string">"label"</span>])
      self.log(<span class="hljs-string">"val_loss"</span>, loss)
      <span class="hljs-keyword">return</span> loss

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">configure_optimizers</span><span class="hljs-params">(self)</span>:</span>
      <span class="hljs-string">"""
      Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is
      usually useful for training video models.
      """</span>
      <span class="hljs-keyword">return</span> torch.optim.Adam(self.parameters(), lr=<span class="hljs-number">1e-1</span>)
</code></pre>
<p>Our VideoClassificationLightningModule and KineticsDataModule are ready be trained together using the <a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html">pytorch_lightning.Trainer</a>!. The trainer class has many arguments to define the training environment (e.g. num_gpus, distributed_backend). To keep things simple we'll just use the default local cpu training but note that this would likely take weeks to train so you might want to use more performant settings based on your environment.</p>
<pre><code class="hljs css language-python">  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>
    classification_module = VideoClassificationLightningModule()
    data_module = KineticsDataModule()
    trainer = pytorch_lightning.Trainer()
    trainer.fit(classification_module, data_module)
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h1>
<p>In this tutorial we showed how to train a 3D ResNet on Kinetics using PyTorch Lightning. You can see the final code from the tutorial (including a few extra bells and whistles) in the PyTorchVideo projects directory.</p>
<p>To learn more about PyTorchVideo, check out the rest of the <a href="http://pytorchvideo.org/docs/api/index.html">documentation</a>  and <a href="http://pytorchvideo.org/docs/tutorial_overview">tutorials</a>.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/ptv_tempdocs/tutorial_overview"><span class="arrow-prev">← </span><span>Overview</span></a><a class="docs-next button" href="/ptv_tempdocs/doc_accelerator_overview"><span>Overview</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div class="footerSection"><div class="social"><a class="github-button" href="https://github.com/facebookresearch/pytorchvideo" data-count-href="https://github.com/facebookresearch/pytorchvideo/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star PytorchVideo on GitHub">pytorchvideo</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/ptv_tempimg/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2021 Facebook, Inc<br/>Legal:<a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></section></footer></div></body></html>